@article{Beckingsale:2019,
abstract = {Modern high-performance computing systems are diverse, with hardware designs ranging from homogeneous multi- core CPUs to GPU or FPGA accelerated systems. Achieving desir- able application performance often requires choosing a program- ming model best suited to a particular platform. For large codes used daily in production that are under continual development, architecture-specific ports are untenable. Maintainability re- quires single-source application code that is performance portable across a range of architectures and programming models. In this paper we describe RAJA, a portability layer that enables C++ applications to leverage various programming models, and thus architectures, with a single-source codebase. We describe preliminary results using RAJA in three large production codes at Lawrence Livermore National Laboratory, observing 17×, 13× and 12× speedup on GPU-only over CPU- only nodes with single-source application code in each case.},
author = {Beckingsale, David A. and Scogland, Thomas R.W. and Burmark, Jason and Hornung, Rich and Jones, Holger and Killian, William and Kunen, Adam J. and Pearce, Olga and Robinson, Peter and Ryujin, Brian S.},
doi = {10.1109/P3HPC49587.2019.00012},
file = {:Users/settgast1/Documents/Mendeley Desktop/Beckingsale et al/2019/RAJA Portable Performance for Large-Scale Scientific Applications/Beckingsale et al. - 2019 - RAJA Portable Performance for Large-Scale Scientific Applications.pdf:pdf},
isbn = {9781728160030},
journal = {Proceedings of P3HPC 2019: International Workshop on Performance, Portability and Productivity in HPC - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {71--81},
title = {{RAJA: Portable Performance for Large-Scale Scientific Applications}},
year = {2019}
}

@misc{CHAI:2023,
  title = {CHAI},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/LLNL/chai}
}

@article{Beckingsale:2020,
abstract = {Advanced architectures like Sierra provide a wide range of memory resources that must often be carefully controlled by the user. These resources have varying capacities, access timing rules, and visibility to different compute resources. Applications must intelligently allocate data in these spaces, and depending on the total amount of memory required, applications may also be forced to move data between different parts of the memory hierarchy. Finally, applications using multiple packages must coordinate effectively to ensure that each package can use the memory resources it needs. To address these challenges, we present Umpire, an application-oriented library for managing memory resources. Specifically, Umpire provides support for querying memory resources, provisioning and allocating memory, and memory introspection. It allows computer scientists and computational physicists to efficiently program the memory hierarchies of current and future high-performance computing architectures, without tying their application to specific hardware or software. In this article, we describe the design and implementation of Umpire and present case studies from the integration of Umpire into applications that are currently running on Sierra.},
author = {Beckingsale, D. A. and McFadden, M. J. and Dahm, J. P.S. and Pankajakshan, R. and Hornung, R. D.},
doi = {10.1147/JRD.2019.2954403},
file = {:Users/settgast1/Documents/Mendeley Desktop/Beckingsale et al/2020/Umpire Application-focused management and coordination of complex hierarchical memory/Beckingsale et al. - 2020 - Umpire Application-focused management and coordination of complex hierarchical memory.pdf:pdf},
issn = {21518556},
journal = {IBM Journal of Research and Development},
number = {3-4},
pages = {1--10},
title = {{Umpire: Application-focused management and coordination of complex hierarchical memory}},
volume = {64},
year = {2020}
}

@misc{hypre,
  key   =       {hypre},
  title =       {{\sl hypre}: High Performance Preconditioners},
  journal = {GitHub repository},
  url = {\url{https://llnl.gov/casc/hypre}, \url{https://github.com/hypre-space/hypre}}
  }







@article{BUI:2020,
author = {Bui, Quan M. and Osei-Kuffuor, Daniel and Castelletto, Nicola and White, Joshua A.},
title = {A Scalable Multigrid Reduction Framework for Multiphase Poromechanics of Heterogeneous Media},
journal = {SIAM Journal on Scientific Computing},
volume = {42},
number = {2},
pages = {B379-B396},
year = {2020},
doi = {10.1137/19M1256117},
URL = {https://doi.org/10.1137/19M1256117},
eprint = {https://doi.org/10.1137/19M1256117},
abstract = { Simulation of multiphase poromechanics involves solving a multiphysics problem in which multiphase flow and transport are tightly coupled with the porous medium deformation. To capture this dynamic interplay, fully implicit methods, also known as monolithic approaches, are usually preferred. The main bottleneck of a monolithic approach is that it requires solution of large linear systems that result from the discretization and linearization of the governing balance equations. Because such systems are nonsymmetric, indefinite, and highly ill-conditioned, preconditioning is critical for fast convergence. Recently, most efforts in designing efficient preconditioners for multiphase poromechanics have been dominated by physics-based strategies. Current state-of-the-art “black-box” solvers such as algebraic multigrid (AMG) are ineffective because they cannot effectively capture the strong coupling between the mechanics and the flow subproblems, as well as the coupling inherent in the multiphase flow and transport process. In this work, we develop an algebraic framework based on multigrid reduction (MGR) that is suited for tightly coupled systems of PDEs. Using this framework, the decoupling between the equations is done algebraically through defining appropriate interpolation and restriction operators. One can then employ existing solvers for each of the decoupled blocks or design a new solver based on knowledge of the physics. We demonstrate the applicability of our framework when used as a “black-box” solver for multiphase poromechanics. We show that the framework is flexible to accommodate a wide range of scenarios, as well as efficient and scalable for large problems. }
}

@article{BUI:2021114111,
title = {Multigrid reduction preconditioning framework for coupled processes in porous and fractured media},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {387},
pages = {114111},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114111},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521004424},
author = {Quan M. Bui and François P. Hamon and Nicola Castelletto and Daniel Osei-Kuffuor and Randolph R. Settgast and Joshua A. White},
keywords = {Preconditioning, Algebraic multigrid, Hydraulic fracturing, Compositional flow, Mimetic finite difference method},
abstract = {Many subsurface engineering applications involve tight-coupling between fluid flow, solid deformation, fracturing, and similar processes. To better understand the complex interplay of different governing equations, and therefore design efficient and safe operations, numerical simulations are widely used. Given the relatively long time-scales of interest, fully-implicit time-stepping schemes are often necessary to avoid time-step stability restrictions. A major computational bottleneck for these methods, however, is the linear solver. These systems are extremely large and ill-conditioned. Because of the wide range of processes and couplings that may be involved – e.g. formation and propagation of fractures, deformation of the solid porous medium, viscous flow of one or more fluids in the pores and fractures, complicated well sources and sinks, etc. – it is difficult to develop general-purpose but scalable linear solver frameworks. This challenge is further aggravated by the range of different discretization schemes that may be adopted, which have a direct impact on the linear system structure. To address this obstacle, we describe a flexible strategy based on multigrid reduction (MGR) that can produce purely algebraic preconditioners for a wide spectrum of relevant physics and discretizations. We demonstrate that MGR, guided by physics and theory in block preconditioning, can tackle several distinct and challenging problems, notably: a hybrid discretization of single-phase flow, compositional multiphase flow with complex wells, and hydraulic fracturing simulations. Extension to other systems can be handled quite naturally. We demonstrate the efficiency and scalability of the resulting solvers through numerical examples of difficult, field-scale problems.}
}

@article{Pearson:2017,
  	url = {http://adsabs.harvard.edu/abs/2017arXiv170304627P},
  	Archiveprefix = {arXiv},
  	Author = {{Pearson}, S. and {Price-Whelan}, A.~M. and {Johnston}, K.~V.},
  	Eprint = {1703.04627},
  	Journal = {ArXiv e-prints},
  	Keywords = {Astrophysics - Astrophysics of Galaxies},
  	Month = mar,
  	Title = {{Gaps in Globular Cluster Streams: Pal 5 and the Galactic Bar}},
  	Year = 2017
}

@book{Binney:2008,
  	url = {http://adsabs.harvard.edu/abs/2008gady.book.....B},
  	Author = {{Binney}, J. and {Tremaine}, S.},
  	Booktitle = {Galactic Dynamics: Second Edition, by James Binney and Scott Tremaine.~ISBN 978-0-691-13026-2 (HB).~Published by Princeton University Press, Princeton, NJ USA, 2008.},
  	Publisher = {Princeton University Press},
  	Title = {{Galactic Dynamics: Second Edition}},
  	Year = 2008
}

@article{gaia,
    author = {{Gaia Collaboration}},
    title = "{The Gaia mission}",
    journal = {Astronomy and Astrophysics},
    archivePrefix = "arXiv",
    eprint = {1609.04153},
    primaryClass = "astro-ph.IM",
    keywords = {space vehicles: instruments, Galaxy: structure, astrometry, parallaxes, proper motions, telescopes},
    year = 2016,
    month = nov,
    volume = 595,
    doi = {10.1051/0004-6361/201629272},
    url = {http://adsabs.harvard.edu/abs/2016A%26A...595A...1G},
}

@article{astropy,
    author = {{Astropy Collaboration}},
    title = "{Astropy: A community Python package for astronomy}",
    journal = {Astronomy and Astrophysics},
    archivePrefix = "arXiv",
    eprint = {1307.6212},
    primaryClass = "astro-ph.IM",
    keywords = {methods: data analysis, methods: miscellaneous, virtual observatory tools},
    year = 2013,
    month = oct,
    volume = 558,
    doi = {10.1051/0004-6361/201322068},
    url = {http://adsabs.harvard.edu/abs/2013A%26A...558A..33A}
}

@misc{fidgit,
  author = {A. M. Smith and K. Thaney and M. Hahnel},
  title = {Fidgit: An ungodly union of GitHub and Figshare},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/arfon/fidgit}
}